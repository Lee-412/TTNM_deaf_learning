\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{vietnam}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{array}
\usepackage{amssymb}
\usepackage{longtable}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{xcolor} 
\usepackage{graphicx}
\usepackage[margin=1in]{geometry} 
\usepackage[font=small]{caption}
\usepackage{booktabs}
\usepackage[unicode]{hyperref}
\usepackage{titlesec} 
\usepackage{scrextend}
\usepackage{enumerate}
\usepackage{url}
\usepackage{tikz}
\usepackage{float}
\usepackage{afterpage}
\usepackage{multirow}
\usepackage{sectsty}
\usepackage{tocloft,calc}
\usepackage{listings}
\usepackage{makecell}
\usepackage[sort&compress]{natbib}
\usepackage{algorithm}
\usepackage{bookmark}
\usepackage{subcaption}
\usepackage[flushleft]{threeparttable}
\usepackage{perpage}
\newcommand{\Emptyset}{\text{\o}}
\usepackage{lmodern}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{multicol}
\newcommand{\tab}{\hspace{5\fontdimen2\font}}
\usepackage{wrapfig}
\usepackage{afterpage}
\usepackage{longtable}

\linespread{1.25}
\newcommand\myemptypage{
    \null
    \thispagestyle{empty}
    \addtocounter{page}{-1}
    \newpage
    }
\usetikzlibrary{calc}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage[style=authoryear,backend=biber]{biblatex} % Use 'authoryear' or other styles
\addbibresource{references.bib} % Link your .bib file
\AtNextBibliography{\small}

\begin{document}

%Bìa here
\thispagestyle{empty}

\begin{tikzpicture}[remember picture, overlay]
  \draw[line width = 2pt] ($(current page.north west) + (0.5in,-0.5in)$) rectangle ($(current page.south east) + (-0.5in,0.5in)$);
\end{tikzpicture}

\begin{center}
\changefontsizes{16pt}

	\textbf{ĐẠI HỌC QUỐC GIA HÀ NỘI\\TRƯỜNG ĐẠI HỌC CÔNG NGHỆ}\\[1cm]
	\includegraphics[width=0.4\linewidth]{images/uet.png}\\[1cm]
\changefontsizes{14pt}
        \textbf{BÁO CÁO BÀI TẬP LỚN CUỐI KỲ BỘ MÔN TƯƠNG TÁC NGƯỜI MÁY}
        \\
        \textbf{NĂM HỌC 2024 – 2025}
        \\[1cm]
        \textbf{NHÓM 1}
	\\[1cm]
\changefontsizes{13pt}
        \begin{tabular}{l @{\hskip 0.5in} l}
            Giảng viên hướng dẫn & TS. Ngô Thị Duyên \\
            Môn học & Tương tác người máy \\
            Lớp học phần & INT2041 55 \\
            Thành viên & Tăng Vĩnh Hà - 22028129 \\
            & Nguyễn Hữu Thế - 22028155 \\
            & Vũ Thị Minh Thư - 22028116 \\
            & Chu Quang Cần - 22028093 \\
            & Lê Xuân Hùng - 22028172 \\
        \end{tabular}
	\vfill
	\textbf{HÀ NỘI - 2024}
\end{center}
\clearpage

%Mục lục here
\tableofcontents{}
\clearpage


\section{Giới thiệu bài toán}

\subsection{Bối cảnh và động lực}
Ngôn ngữ ký hiệu hay còn gọi là thủ ngữ là phương tiện giao tiếp vô cùng quan trọng đối với cộng đồng người điếc và khiếm thính. Không những thế, đối với những người đối diện, người nghe, những người gặp những vấn đề về nói, cũng như những người muốn cảm thông, quan tâm và giao tiếp với họ. Thủ ngữ cũng là một con đường có thể giúp đỡ và gắn kết mọi người với nhau hơn.

\noindent Tuy nhiên, việc học tập ngôn ngữ ký hiệu vẫn gặp nhiều trở ngại do thiếu sách giáo khoa chất lượng, phương pháp dạy hạn chế và sự hỗ trợ hạn chế từ công nghệ thông tin. Với sự phát triển không ngừng của công nghệ di động và trí tuệ nhân tạo, việc áp dụng các công nghệ này vào việc học ngôn ngữ ký hiệu mang lại nhiều cơ hội để cải thiện trải nghiệm học tập cho người điếc.

\noindent Trên cơ sở đó, SignLearn ra đời với mục đích hỗ trợ người dùng, giúp đỡ phần nào trong việc khó khăn về tiếp cận nội dung học tập, tài liệu liên quan, cộng đồng – xã hội cùng chung hoàn cảnh và điều kiện.


\subsection{Mục tiêu của dự án}
Dự án hướng tới xây dựng một trang web hỗ trợ và giúp mọi người có thể giao tiếp dễ dàng hơn với người khiếm thính thông qua các khía cạnh:
\begin{itemize}
    \item \textbf{Phát triển website} hỗ trợ học ngôn ngữ ký hiệu cho người câm điếc, người có nhu cầu học ký hiệu thủ ngữ cho mục đích giao tiếp.

    \item \textbf{Cung cấp các tài liệu học tập}, nguồn tài liệu phong phú bao gồm video, hình ảnh và các bài báo, trang web cộng đồng hữu ích
    
    \item \textbf{Theo dõi tiến độ}, giúp những người liên quan theo dõi tiến độ người dùng và đưa ra phản hồi cá nhân hóa phù hợp.
    
    \item \textbf{Tạo ra một cộng đồng hỗ trợ} nơi người dùng có thể nhận được sự giúp đỡ từ các chuyên gia và người dùng khác.
\end{itemize}


\subsection{Phạm vi và giới hạn của dự án}
\noindent \textbf{Phạm vi}: Ứng dụng website tập trung vào việc học ngôn ngữ ký hiệu Việt Nam.

\noindent \textbf{Giới hạn}: Dự án chưa bao gồm việc phát triển ứng dụng cho phiên bản cho máy tính bảng hoặc thiết bị đeo thông minh. Chỉ hỗ trợ cho việc sử dụng trang web ở các layout cho thiết bị di động và máy tính bảng. Chưa có đột phá về mặt trí tuệ nhân tạo, chỉ nằm ở mức phân tích cử chỉ, ký hiệu và dự đoán kết quả dựa trên dữ liệu đã training.

\section{Các công trình liên quan}

\subsection{Các sản phẩm tương tự trên thị trường}
Sau khi xác định được đối tượng hỗ trợ là người khiếm thị, nhóm đã đi tìm hiểu và tham khảo các phần mềm hỗ trợ đối tượng nói trên có trên thị trường. Đã có nhiều phần mềm hỗ trợ người khuyết tật về thị giác như \textbf{Seeing AI}, phần mềm do Microsoft phát triển, sử dụng trí tuệ nhân tạo để mô tả môi trường xung quanh, đọc văn bản, nhận diện khuôn mặt và vật thể thông qua giọng nói. Hay phần mềm \textbf{KNFB Reader} \cite{dadhich2018real} cho phép người dùng chụp ảnh tài liệu và chuyển đổi chúng thành văn bản âm thanh hoặc văn bản kỹ thuật số, hỗ trợ việc đọc nhanh chóng. Phần mềm \textbf{Voice Dream Reader} là một ứng dụng đọc văn bản đa dạng từ sách, tài liệu đến email, hỗ trợ người khiếm thị tiếp cận thông tin dễ dàng hơn.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/seeingai.png}
    \caption{Minh họa các chức năng của ứng dụng SeeingAI như: nhận dạng đồ vật và text recognition, v.v.}
\end{figure}
Các phần mềm kể trên đều hỗ trợ người khiếm thị ở nhiều khía cạnh khác nhau của cuộc sống bằng cách cung cấp các nhóm chức năng khác nhau như: tìm đồ vật, đọc văn bản, v.v. Tuy nhiên, nhóm nhận định rằng các sản phẩm tuy ứng dụng rất thiết thực và có ích cho người khiếm thị, tuy nhiên chưa thực sự có giao diện thân thiện - dễ sử dụng với đối tượng mà họ hướng tới. 
\begin{figure}[H]
    \centering
    \begin{minipage}{0.495\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/docreader.jpg}
        \caption{Minh họa chức năng chuyển đổi văn bản của KNFB Reader}
    \end{minipage}
    \hfill
    \begin{minipage}{0.495\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/VoiceDreamReader.jpg}
        \caption{Minh họa chức năng đọc văn bản của Voice Dream Reader}
    \end{minipage}
\end{figure}
Và sau khi tham khảo, trong phạm vi và thời gian cho phép của môn học, nhóm đã quyết định mục tiêu của dự án của nhóm là hỗ trợ người khiếm thị trong hai khía cạnh: tìm kiếm đồ vật và nhận biết môi trường xung quanh; giao tiếp và tương tác xã hội. Đồng thời, nhóm cũng xác định thiết kế giao diện thân thiện hơn với người dùng khiếm thị so với các phần mềm nói trên bằng cách cho họ tương tác với phần mêm thông qua ba thao tác chính là: chạm, vuốt màn hình và nói. Từ đó, nhóm tiến hành nghiên cứu 3 bài toán của lĩnh vực thị giác máy, trí tuệ nhân tạo tạo sinh để xây dựng sản phẩm như sau.

\subsection{Các ứng dụng của trí tuệ nhân tạo trong hỗ trợ người khiếm thị}

\subsubsection{Bài toán object detection}
Object detection là một bài toán trong lĩnh vực thị giác máy tính, với mục tiêu xác định vị trí và phân loại các vật thể trong hình ảnh hoặc video. Input của bài toán là một hình ảnh hoặc một khung hình video, output là danh sách các vật thể được nhận diện kèm theo nhãn và tọa độ các hộp giới hạn (bounding boxes) bao quanh chúng. Trong hỗ trợ người khiếm thị, bài toán này giúp phát hiện và nhận diện các vật thể trong môi trường xung quanh, cải thiện khả năng di chuyển và an toàn. Một số mô hình nổi bật trong lĩnh vực object detection bao gồm \textbf{YOLO} (\cite{redmon2016you}) với khả năng phát hiện nhanh và \textbf{SSD} (\cite{liu2016ssd}) có tốc độ và độ chính xác cao. Tuy nhiên, đối với các ứng dụng cho thiết bị di động, \textbf{MobileNet} (\cite{howard2017mobilenets}) được ưu tiên vì kích cỡ nhỏ, khả năng tối ưu tài nguyên và phù hợp hơn. Dự án của nhóm sẽ sử dụng mô hình \textbf{MobileNet} cho chức năng 1. Chi tiết hơn về mô hình sẽ được trình bày ở phần 3.3 công nghệ sử dụng.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/objectdetection.png}
    \caption{Minh họa cho chức năng object recognition, ngoài việc đọc tên đồ vật - phần mềm còn có thể nói vị trí của đồ vật so với người dùng là ở bên trái, bên phải,..}
\end{figure}
\subsubsection{Bài toán visual question answering}
Visual Question Answering (VQA) là một bài toán huấn luyện máy để thực hiện tương tác hỏi đáp giữa người và máy về một hình ảnh trực quan. VQA kết hợp giữa thị giác máy tính và xử lý ngôn ngữ tự nhiên, trong đó hệ thống nhận input là một hình ảnh và một câu hỏi liên quan đến hình ảnh đó, sau đó trả về câu trả lời dưới dạng văn bản.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/VQA.png}
    \caption{Minh họa cho VQA}
    \label{fig:enter-label}
\end{figure}

Một số mô hình VQA nổi tiếng hiện nay bao gồm \textbf{PaliGemma} (\cite{beyer2024paligemma}), \textbf{LXMERT} (\cite{tan2019lxmert}) và \textbf{VilBERT}(\cite{lu2019vilbert}), v.v. thường được xây dựng từ các mô hình ngôn ngữ lớn và được tối ưu hóa để cung cấp các phản hồi chính xác dựa trên thông tin về mặt hình ảnh. Trong các ứng dụng hỗ trợ người khiếm thị, VQA giúp họ tìm hiểu môi trường xung quanh thông qua tương tác bằng câu hỏi, câu trả lời và cung cấp thông tin hữu ích dựa trên câu hỏi của người dùng về bức ảnh đầu vào. Bài toán này sẽ giúp nhóm triển khai chức năng khám phá môi trường xung quanh - chức năng 2. Chức năng này sẽ sử dụng mô hình \textbf{PaliGemma}, kết hợp giữa công nghệ thị giác máy và xử lý ngôn ngữ tự nhiên, cho phép cung cấp các mô tả chi tiết (image captioning) và trả lời câu hỏi dựa trên hình ảnh (visual question answering). Quy trình hoạt động bắt đầu khi người dùng chụp ảnh khu vực cần mô tả; hệ thống sẽ kiểm tra chất lượng ảnh và yêu cầu chụp lại nếu ảnh không đạt yêu cầu. Sau đó, người dùng đặt câu hỏi qua giọng nói, hệ thống sử dụng mô hình PaliGemma để xử lý ảnh và câu hỏi, từ đó đưa ra câu trả lời chính xác. Phản hồi cuối cùng sẽ hiển thị dưới dạng văn bản và được chuyển thành giọng nói để người dùng tiếp nhận dễ dàng. Chi tiết hơn về mô hình sẽ được trình bày ở phần 3.3 công nghệ sử dụng.

\subsubsection{Bài toán face recognition và emotion recognition}
Face recognition là bài toán mục tiêu xác định hoặc xác thực danh tính của một người dựa trên khuôn mặt. Input của bài toán là hình ảnh hoặc video chứa khuôn mặt, và output là danh tính của người đó (nếu đã được lưu trong cơ sở dữ liệu) hoặc xác nhận khuôn mặt trùng khớp với danh tính đã biết. Trong ứng dụng di động hỗ trợ người khiếm thị, công nghệ này có thể có nhiều vai trò: giúp người khiếm thị xác định danh tính người quen trong các cuộc trò chuyện để cải thiện sự tự tin và khả năng tương tác xã hội; hay bảo vệ người khiếm thị bằng cách cảnh báo khi có người lạ tiếp cận, đặc biệt trong các tình huống nhạy cảm như giao dịch tài chính hoặc khi đi lại ở nơi công cộng. 
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/FR and ER.png}
    \caption{Minh họa cho face recognition và emotion recognition}
    \label{fig:enter-label}
\end{figure}

Ngoài ra, bài toán này còn có thể phát triển thành nhiều cấp độ khác như: emotion recognition (nhận diện cảm xúc) giúp người khiếm thị hiểu được trạng thái cảm xúc của người đối diện, từ đó hỗ trợ họ trong các cuộc trò chuyện và giao tiếp hiệu quả hơn. Ví dụ, việc nhận biết biểu cảm như vui vẻ, lo lắng, hay giận dữ sẽ giúp người khiếm thị điều chỉnh cách tương tác phù hợp, tạo ra một môi trường giao tiếp hòa hợp và thân thiện hơn. Đã có nhiều nghiên cứu đáng chú ý về phát triển chức năng emotion recognition trong các ứng dụng hỗ trợ người khiếm thị như các nghiên cứu của \cite{akhand2021facial}, \cite{lutfallah2022emotion}. Input của bài toán emotion recognition sẽ là bức ảnh chụp ảnh khuôn mặt của một người và output sẽ là nhãn cảm xúc của khuôn mặt đó trong bức ảnh.

Hai bài toán này sẽ lần lượt giúp giải quyết chức năng 3 và 4. Nhóm sử dụng hai công nghệ: \textbf{API Face Detection} của \textbf{Google ML Kit} và mô hình \textbf{MobileFaceNet}.  Đối với bài toán của chức năng 3, pipeline sẽ gồm hai công việc tuần tự: phát hiện khuôn mặt và nhận diện khuôn mặt. Module phát hiện khuôn mặt sẽ dùng API Face Detection của Google ML Kit để vẽ xác định bounding box hay vị trí của khuôn mặt có trong bức ảnh. Sau khi xác định được boundingbox quanh khuôn mặt, ảnh sẽ được scale và cắt xung quanh boundingbox để trở thành ảnh dạng 112x112 px chỉ gồm thông tin khuôn mặt được dùng làm input cho module nhận diện khuôn mặt. Module nhận diện khuôn mặt sẽ dùng mô hình MobileFaceNet và với input như đã nêu, output của model MobileFaceNet sẽ là nhãn dán tên / biệt danh của người trong ảnh hoặc là không phản hồi nếu là người lạ. Đối với chức năng 4, nhóm trực tiếp sử dụng chức năng emotion recognition do API Face Detection của Google ML Kit cung cấp. Chi tiết hơn về mô hình sẽ ở phần 3.3 công nghệ sử dụng.


\section{Đề xuất giải pháp}
\subsection{Đối tượng hướng đến}
Ứng dụng được thiết kế dành riêng cho người khiếm thị – những người gặp khó khăn trong sinh hoạt hàng ngày do không thể nhận diện không gian xung quanh cũng như trong giao tiếp hằng ngày khi họ có thể quan sát được người đối diện và biểu cảm của đối phương đang giao tiếp. Hệ thống nhóm xây dựng cung cấp bộ chức năng hỗ trợ tương tác với môi trường xung quanh gồm: xác định đồ vật và mô tả môi trường xung quanh để giúp người dùng sinh hoạt hàng ngày thuận lợi hơn. Bên cạnh đó, hệ thống cũng cung cấp bộ chức năng hỗ trợ người dùng trong giao tiếp hằng ngày bao gồm: nhận dạng khuôn mặt người thân và nhận dạng cảm xúc giúp người khiếm thị giao tiếp hiệu quả và độc lập hơn trong cuộc sống hằng ngày.

\subsection{Các chức năng mà ứng dụng cung cấp}
Ứng dụng được chia ra làm hai chế độ chính là: chế độ \textbf{Explore} gồm 2 chức năng \textbf{Detect}, \textbf{Explore} và chế độ \textbf{Socializing} gồm 2 chức năng \textbf{Face Recognition}, \textbf{Mood Tracking}.

\subsubsection{Chức năng 1: Xác định toàn bộ vật thể xung quanh}
Chức năng thứ nhất của ứng dụng là \textbf{Detect}, xác định các đồ vật xung quanh người dùng, cung cấp thông tin chi tiết về môi trường xung quanh nhằm hỗ trợ khả năng tương tác trong các hoạt động hàng ngày. Nhóm sử dụng mô hình đã được huấn luyện sẵn MobileNet như đã nhắc ở phía trên để tiết kiệm tối đa dung lượng của hệ thống trên điện thoại di động. Sau khi xác định được các đồ vật xung quanh và vị trí của chúng so với người dùng (trái, phải, v.v), ứng dụng sẽ sử dụng module chuyển văn bản thành giọng nói (text-to-speech) để đọc tên toàn bộ vật thể một cách lần lượt, giúp người dùng có thể hình dung và nắm bắt được thông tin chi tiết về không gian xung quanh họ.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/chucnang11.jpg}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/chucnang12.jpg}
        \label{fig:enter-label}
    \end{minipage}
    \caption{Ảnh giao diện chức năng 1. Các vật xuất hiện trong khung hình sẽ được đọc tên lần lượt để người dùng nghe thấy}

\end{figure}

\subsubsection{Chức năng 2: Mô tả Môi Trường Xung Quanh}
Chức năng thứ 2 của ứng dụng là \textbf{Explore}, cho phép người dùng hỏi đáp về bức ảnh của môi trường xung quanh để giúp họ có cái nhìn tổng thể về không gian hiện tại. Nhóm sử dụng mô hình PaliGemma như đã nhắc ở phía trên. 
\begin{figure}[H]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/explore1.jpg}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/explore2.jpg}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/explore3.jpg}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/explore4.jpg}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/explore5.jpg}
    \end{minipage}
    
    \caption{Ảnh giao diện chức năng 2.}
\end{figure}
Các bước thao tác như sau: màn hình mặc định sẽ là một camera được mở sẵn, người dùng bấm 2 lần để yêu cầu chụp ảnh vào phần dưới của điện thoại. Khi đó ảnh sẽ được chụp và nút camera màu xanh sẽ được hiện ra ở phía dưới màn hình để click chọn nếu muốn chụp ảnh lại. Bấm vào phần dưới (nút camera đó) nếu muốn chụp ảnh lại hoặc vào phần trên màn hình (vị trí bất kì) nếu muốn bắt đầu hỏi. Đọc câu hỏi/yêu cầu của mình. Sau khi đọc xong, hệ thống vào màn hình trạng thái xử lý, sau đó trả lời người dùng qua kênh âm thanh để người dùng nghe thấy và quay lại màn hình chụp xong bức ảnh ban đầu. Người dùng có thể tiến hành hỏi tiếp hoặc chụp bức ảnh mới.
Không giống chức năng 1 chỉ tập trung xác định các đồ vật riêng lẻ, chức năng 2 mang đến mô tả toàn cảnh, tổng hợp các thông tin từ khung cảnh thành một câu văn hoàn chỉnh. Người dùng sẽ chụp một bức ảnh của không gian xung quanh và thao tác với hệ thống bằng cách chạm màn hình hai lần để kích hoạt chế độ ghi âm của module speech-to-text và bắt đầu nói. Khi người dùng dừng nói, hệ thống sẽ gửi yêu cầu tới server API HuggingFace của PaliGemma bức ảnh chụp ngoại cảnh và văn bản được chuyển từ lời nói của người dùng qua module speech-to-text. Khi hệ thống nhận được phản hồi dưới dạng văn bản từ phía server API, ứng dụng sẽ kích hoạt module text-to-speech để chuyển văn bản thành giọng nói cho người dùng nghe. Sau đó, người dùng có thể đặt câu hỏi thêm về hình ảnh với mô hình PaliGemma. Tuy nhiên, trái với chức năng 1 hoạt động theo thời gian thực, chức năng 2 chỉ sử dụng được khi người dùng chụp ảnh môi trường xung quanh.

\subsubsection{Chức năng 3: Nhận diện người quen}
Chức năng thứ 3 của ứng dụng là \textbf{Face Recognition}, nằm trong bộ chức năng giao tiếp xã hội, hỗ trợ người dùng trong các vấn đề giao tiếp với những người xung quanh. Chức năng này sẽ giúp người dùng nhận diện những khuôn mặt được người dùng đăng ký và đặt tên từ trước, sau đó sẽ đọc tên người đó lên. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{images/FR_Cap.png}
    \caption{Ảnh giao diện chức năng 3}
\end{figure}


Để lưu một người vào danh sách người quen, người dùng sẽ chụp ảnh khuôn mặt của họ và chạm vào màn hình hai lần để kích hoạt module speech-to-text để đọc tên người quen đó, sau khi đọc xong thì hệ thống sẽ tự động lưu thông tin vào trong bộ nhớ trên điện thoại. Để nhận dạng người quen, người dùng sẽ đưa camera điện thoại lên trước mặt người cần nhận diện, hệ thống sẽ thu hình ảnh của người cần nhận diện và cho qua pipeline ML Kit, MobileFaceNet như đã mô tả ở phần 3.3.2 để cho ra output là tên người quen đã lưu trong cơ sở dữ liệu hoặc im lặng, không phản hồi gì nếu đó là người lạ. Kế tiếp tên của khuôn mặt đó sẽ được đọc thành âm thanh bởi mô hình text-to-speech tương tự như các tác vụ trên.

\subsubsection{Chức năng 4: Theo dõi cảm xúc trong giao tiếp}
Chức năng thứ 4 của ứng dụng là \textbf{Mood Tracking}, theo dõi trạng thái cảm xúc của người đối diện trong giao tiếp nằm trong bộ chức năng giao tiếp xã hội. Cảm nhận cảm xúc của đối phương rất quan trọng trong việc giao tiếp hiệu quả. Người khiếm thị phần nào bị hạn chế khả năng này khi họ chỉ có thể cảm nhận được cảm xúc của người đối diện thông qua kênh âm thanh mà không thể quan sát biểu cảm khuôn mặt của họ. Đã có nhiều nghiên cứu cho bài toán này như các nghiên cứu: \cite{akhand2021facial}, \cite{lutfallah2022emotion}. Chính vì vậy, chức năng này được cài đặt với mong muốn hỗ trợ người khiếm thị hơn nữa trong việc cảm nhận cảm xúc với người họ đang giao tiếp. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{images/ER.png}
    \caption{Ảnh giao diện chức năng 4}
\end{figure}

Hệ thống mới chỉ đang ở mức xây dựng thử nghiệm chức năng và mới chỉ nhận dạng hai cảm xúc chính: vui vẻ và buồn. Khi người dùng đưa điện thoại lên trước để thu được hình ảnh của người mà họ đang trò chuyện, phần mềm sẽ phát ra một âm thanh vui tươi nếu người đối diện đang vui vẻ và một âm thanh trầm buồn nếu người đối diện đang trong trạng thái không vui. 

\subsection{Công nghệ sử dụng}
Dưới đây là tóm tắt về công nghệ nhóm sử dụng trong dự án:

\begin{table}[h!]
\centering
\begin{tabular}{|p{6cm}|p{8cm}|}
\hline
\textbf{Chức năng} & \textbf{Công nghệ} \\ 
\hline
Cài đặt ứng dụng Android & JetpackCompose và SDK \\ 
\hline
Chức năng 1 - ``Detect'' & Mô hình MobileNet được huấn luyện trên tập dữ liệu COCO. \\ 
\hline
Chức năng 2 - ``Explore'' & API HuggingFace của mô hình PaliGemma \\ 
\hline
Chức năng 3 - ``Face recognition'' & Google ML Kit và MobileFaceNet \\ 
\hline
Chức năng 4 - ``Mood tracking'' & Google ML Kit \\ 
\hline
\end{tabular}
\caption{Tóm tắt công nghệ sử dụng trong dự án}
\label{table:tech_summary}
\end{table}


\subsubsection{Công nghệ xây dựng phần mềm mobile}
Jetpack Compose là bộ công cụ phát triển UI của Google cho ứng dụng Android. Jetpack Compose được thiết kế để đơn giản hóa và tối ưu hóa quy trình xây dựng giao diện nhờ vào cách tiếp cận khai báo (declarative). Với Jetpack Compose, các giao diện phức tạp có thể được tạo ra bằng cách sử dụng các thành phần giao diện như \texttt{Text}, \texttt{Button}, và \texttt{Image} dưới dạng hàm, thay vì phải làm việc trực tiếp với XML như trước đây. Về ứng dụng trong dự án, nhóm sử dụng Jetpack Compose để xây dựng toàn bộ giao diện ứng dụng, từ màn hình chính đến các chức năng phụ trợ như hiển thị thông báo hoặc nhận diện kết quả. 

Android SDK (Software Development Kit) là bộ công cụ phát triển do Google cung cấp để xây dựng ứng dụng Android. Bộ SDK bao gồm nhiều công cụ thiết yếu như trình biên dịch, bộ gỡ lỗi, máy ảo Android Emulator, cùng các API cho phép truy cập sâu vào các thành phần hệ thống như camera, GPS, và cảm biến. Android SDK hỗ trợ đầy đủ các phiên bản hệ điều hành Android, giúp ứng dụng tương thích với nhiều thiết bị. Về ứng dụng trong dự án, Android SDK được nhóm sử dụng để triển khai các chức năng chính của ứng dụng như sau:
\begin{itemize}
    \item Việc tương tác với các cảm biến trên thiết bị và sử dụng các API như \texttt{CameraX} để xử lý hình ảnh từ camera.
    \item Module \texttt{TextToSpeech} cho tác vụ nhận diện vật thể (chức năng 1), hay đọc câu trả lời của hệ thống ở chức năng 2.
    \item Tích hợp với module \texttt{SpeechToText} có sẵn của Google để thu câu hỏi của người dùng ở chức năng 2.
    \item Hỗ trợ tích hợp các mô hình học máy như MobileNet và MobileFaceNet, đảm bảo ứng dụng hoạt động hiệu quả trên các thiết bị Android.
\end{itemize}

\subsubsection{Công nghệ cho bộ chức năng hỗ trợ nhận diện môi trường xung quanh}
Như đã nêu trên, bộ chức năng gồm 1 tác vụ chính: nhận diện và đọc tên đồ vật có trong khung hình và cho người dùng hỏi đáp về môi trường xung quanh. Với chức năng đầu tiên, nhóm sử dụng mô hình MobileNet - một mô hình học sâu được thiết kế tối ưu cho các thiết bị di động nhờ kích thước nhỏ gọn, yêu cầu ít tài nguyên và tốc độ xử lý nhanh. Với kích thước nhỏ, MobileNet vẫn duy trì được độ chính xác cao nhờ vào việc sử dụng các kỹ thuật như depthwise separable convolutions, thay vì các lớp tích chập thông thường, giúp giảm thiểu số lượng tham số và phép toán, đồng thời giữ lại khả năng trích xuất đặc trưng hiệu quả từ dữ liệu hình ảnh. Kiến trúc này cho phép mô hình hoạt động nhanh chóng và tiết kiệm tài nguyên mà vẫn đạt được hiệu suất cao trong các tác vụ nhận diện đối tượng. Về ứng dụng trong dự án, MobileNet được sử dụng trong chức năng 1 như sau: hình được thu từ camera sẽ là input qua mô hình MobileNet, và output của mô hình sẽ là danh sách tên và vị trí bounding box các đồ vật. Chức năng 1 hiện tại nhận dạng được những đồ vật có trong tập nhãn huấn luyện của bộ dữ liệu COCO và thay đổi cách tương tác với người dùng được mô tả ở phần các chức năng phía sau.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/mobilenet.png}
    \caption{Minh họa cấu trúc MobileNet}
    \label{fig:enter-label}
\end{figure}

Với chức năng 2, nhóm sử dụng PaliGemma, mô hình ngôn ngữ - thị giác tiên tiến kết hợp giữa SigLIP (mô hình thị giác) và Gemma (mô hình ngôn ngữ), được phát triển bởi Google thông qua việc gọi API của mô hình đã train từ HuggingFace. Input của mô hình sẽ là: một hình ảnh và một đoạn văn bản, sau đó hình ảnh sẽ được xử lý và được chuyển thành các "soft tokens" (dạng biểu diễn số) bằng SigLIP encoder, còn đoạn văn bản (gọi là prefix) được chuyển thành các token bởi Gemma encoder. Từ đó, các token văn bản và các token hình ảnh được nối lại với nhau, với token văn bản ở phía trước và tất cả các token này được đưa vào Gemma decoder để tạo ra output là văn bản đầu ra. PaliGemma có thể được sử dụng cho nhiều tác vụ như: tạo chú thích (image captioning), và trả lời câu hỏi dựa trên hình ảnh (visual question answering).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/paligemma.png}
    \caption{Minh họa mô hình PaliGemma với 2 đầu vào: hình ảnh và câu hỏi.}
\end{figure}

Về ứng dụng trong dự án, PaliGemma sử dụng như sau: khi người dùng bấm chụp một bức ảnh và hỏi đáp về bức ảnh thông qua kênh âm thanh, nhóm sẽ chuyển đổi câu hỏi đó qua dạng văn bản thông qua module \texttt{speech to text} của Google và gửi bức ảnh, đoạn văn bản đã chuyển đổi đó tới API trên HuggingFace của PaliGemma, sau đó nhận được văn bản output phản hồi từ bên server. Nhóm không tải mô hình về như mô hình MobileNet vì kích thước của mô hình PaliGemma là rất lớn (4GB).

\subsubsection{Công nghệ cho bộ chức năng hỗ trợ giao tiếp và tương tác xã hội}
Như đã nêu trên, bộ chức năng hướng tới hỗ trợ người dùng trong việc giao tiếp và tương tác xã hội gồm hai tác vụ chính: nhận diện người quen (chức năng 3) và nhận diện cảm xúc (chức năng 4). Chức năng 3 sử dụng bộ công cụ Google ML Kit và mô hình MobileFaceNet, chức năng 4 sử dụng bộ công cụ Google ML Kit. 
\\
Về Google ML Kit, đây là bộ công cụ phát triển phần mềm (SDK) được tích hợp vào Firebase. ML Kit cung cấp các công nghệ học máy của Google nhiều năm nghiên cứu cho các mobile developer, cho phép tích hợp một số mô hình học máy vào ứng dụng mobile dễ dàng hơn. ML Kit cung cấp nhiều API cơ sở và custom model support như: \texttt{Text recognition} (nhận dạng văn bản), \texttt{Face detection} (nhận diện khuôn mặt), \texttt{Landmark recognition} (nhận diện mốc), v.v. Google ML Kit hỗ trợ cả hai chế độ trực tuyến và ngoại tuyến. Đối với dự án, nhóm sử dụng tính năng phát hiện khuôn mặt, một chức năng có thể hoạt động ngoại tuyến, đảm bảo quyền riêng tư của người dùng và giảm độ trễ khi xử lý.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/MLToolkit1.jpg}
    \caption{Luồng hoạt động của công cụ xác định khuôn mặt trong MLKit}
    \label{fig:enter-label}
\end{figure}

Về mô hình MobileFaceNet, đây là sự kết hợp giữa kiến trúc MobileNet và các cải tiến dành riêng cho nhận diện khuôn mặt, giúp giảm kích thước mô hình và số lượng phép tính cần thiết mà vẫn duy trì độ chính xác cao. Mô hình sử dụng các khối tích chập hiệu quả như MobileNet để trích xuất đặc trưng từ ảnh khuôn mặt, đồng thời áp dụng một phiên bản cải tiến của hàm tổn thất Softmax (ArcFace) nhằm tăng cường khả năng phân biệt giữa các lớp khuôn mặt trong không gian biểu diễn. MobileFaceNet có kích thước nhỏ gọn chỉ khoảng 5.2 MB, nhưng vẫn duy trì độ chính xác tương đương với các mô hình lớn hơn như FaceNet.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/face_net.png}
    \caption{Kiến trúc mô hình FaceNet}
    \label{fig:enter-label}
\end{figure}

Ứng dụng của Google ML Kit và MobileFaceNet trong chức năng 3 và 4 được thể hiện như sau:
\begin{itemize}
    \item \textbf{Chức năng 3 - nhận diện người quen:} Trong tác vụ này, vai trò của Google ML Kit là xác định vị trí các khuôn mặt có trong khung hình, và tạo ra các bounding box xung quanh các khuôn mặt đó. Từ đó, cung cấp đầu ra là dữ liệu hình ảnh bản cắt các khuôn mặt có trong hình. Mô hình MobileFaceNet sẽ nhận đầu vào là ảnh cắt riêng phần khuôn mặt trong hình (do tác vụ trên cung cấp), sau đó sẽ trả về đầu ra là tên của khuôn mặt đó (nếu khuôn mặt được lưu trong cơ sở dữ liệu từ trước bởi người dùng).
    \item \textbf{Chức năng 4 - mood tracking:} Nhóm trực tiếp sử dụng API \texttt{Face recognition}. API cung cấp chức năng nhận dạng biểu cảm trên khuôn mặt, xác định xem một người đang cười hay nhắm mắt bằng cách định vị các đặc điểm trên khuôn mặt như: tọa độ của mắt, tai, má, mũi và miệng của khuôn mặt được phát hiện để tính toán.
\end{itemize}

\section{Xây dựng sản phẩm}

\subsection{Thiết kế giao diện}

Do đây là ứng dụng hướng tới một nhóm người đặc biệt, người khiếm thị, nên nhóm hướng tới việc thiết kế một giao diện đơn giản và dễ dàng sử dụng kể cả khi người dùng không nhìn vào màn hình điện thoại. Giao diện của nhóm gồm các điểm chính như sau:

\begin{itemize}
    \item Người dùng sẽ tương tác với phần mềm bằng các thao tác chính: vuốt, chạm màn hình và nói. Phần mềm sẽ tương tác với người dùng qua hai kênh chính là: âm thanh. Vuốt ngang để chuyển sang các chức năng cùng chế độ (cùng chế độ là Explore và Detect hoặc Socialize) và vuốt dọc để chuyển sang chế độ khác. Chạm màn hình để thực hiện các thao tác như chụp ảnh, nói. Phần mềm sẽ tương tác, gửi thông tin tới cho người dùng qua kênh âm thanh.
    \item Ngay khi khởi chạy ứng dụng, chức năng 1 \textbf{Detect} sẽ được bật lên, đây là chức năng mặc định của ứng dụng. Người dùng chuyển sang chức năng khác trong cùng bộ chức năng hỗ trợ khám phá xung quanh bằng cách vuốt sang vuốt sang phải hoặc trái. Giao diện của chức năng 1 là một màn hình camera thông thường, nhưng sẽ hiển thị thêm các bounding box của các vật thể.
    \item Chức năng 2 màn hình ban đầu cũng sẽ là màn hình camera, và sau khi người dùng nhấn 2 lần để chụp thì giao diện sẽ chuyển sang chế độ ảnh tĩnh. Sau đó, người dùng nhấn 2 lần để thực hiện đặt câu hỏi thông qua giọng nói, giao diện sẽ ở chế độ chờ ghi âm. Cuối cùng, giao diện sẽ hiện lên phản hồi từ ứng dụng (ở dạng văn bản in dưới màn hình) và ứng dụng sẽ đọc văn bản này bằng module text-to-speech.
    \item Chức năng 3, 4 màn hình ban đầu cũng sẽ là màn hình camera và người dùng sẽ đưa camera về phía người cần nhận diện danh tính hoặc nhận diện cảm xúc.
\end{itemize}
\subsection{Phân tích và đặc tả yêu cầu}
\subsubsection{Chức năng 1: Xác định toàn bộ vật thể xung quanh}
Chức năng phát hiện đồ vật giúp người dùng nhận diện các đồ vật xung quanh bằng cách xác định và đọc tên từng đồ vật trong khung hình camera, hỗ trợ đặc biệt cho người khiếm thị trong việc nhận biết môi trường.
\\
\textbf{Người thực hiện}: Người dùng ứng dụng (đặc biệt là người khiếm thị hoặc những người gặp khó khăn trong việc nhận diện môi trường).
\\
\textbf{Mục tiêu}: Giúp người dùng nhận diện và xác định được các đồ vật xung quanh trong khung hình camera bằng cách đọc tên từng đồ vật kèm vị trí của chúng.
\\
\textbf{Tiền điều kiện}:
\begin{itemize}
    \item Người dùng mở ứng dụng và đã cấp đầy đủ quyền (truy cập camera, microphone).
    \item Người dùng chọn chế độ phát hiện đồ vật thông qua việc vuốt màn hình sang ngang.
\end{itemize}
\\
\textbf{Luồng sự kiện chính}:
\\
\begin{longtable}{|p{6cm}|p{8cm}|}
\hline
\textbf{Hành động của người dùng} & \textbf{Hành động của hệ thống} \\
\hline
1. Người dùng khởi động ứng dụng và ứng dụng ở chế độ phát hiện vật thể (mặc định). & 
2. Ứng dụng mở và chuyển sang chế độ phát hiện đồ vật (chế độ mặc định). Hệ thống thông báo đã sẵn sàng qua âm thanh hoặc tín hiệu. \\
\hline
3. Người dùng ra lệnh chọn chế độ phát hiện đồ vật qua thao tác vuốt từ phải sang trái một lần. (khi hệ thống đang ở chức năng Explore Destination)  & 
4. Hệ thống xác nhận yêu cầu của người dùng và thông báo chế độ đã được chọn thành công bằng âm thanh. \\
\hline
5. Người dùng giơ điện thoại lên, hướng camera đến khu vực muốn quét để phát hiện đồ vật. & 
6. Hệ thống kích hoạt camera và bắt đầu quét môi trường xung quanh, liên tục tìm kiếm các đồ vật trong khung hình. \\
\hline
& 7. Hệ thống nhận diện các đồ vật trong khung hình, vẽ các bounding box xung quanh chúng và xác định tên từng đồ vật (nếu nhận diện được). \\
\hline
& 8. Hệ thống lần lượt đọc tên các đồ vật đã nhận diện cùng với vị trí của chúng trong khung hình (ví dụ: "A chair in front", "A table in the right"). \\
\hline
9. Người dùng nghe các mô tả về các đồ vật và có thể nhận diện được môi trường xung quanh mình. & 
10. Khi người dùng di chuyển camera hoặc phát hiện đồ vật mới, hệ thống tiếp tục cập nhật và mô tả đồ vật mới trong khung hình. \\
\hline
11. Người dùng có thể yêu cầu dừng chế độ phát hiện đồ vật bất kỳ lúc nào. & 
12. Hệ thống dừng chế độ phát hiện đồ vật và thông báo cho người dùng rằng chế độ đã kết thúc. \\
\hline
\end{longtable}
\\
\textbf{Tần suất sử dụng}: Chức năng này thường được sử dụng khi người dùng muốn nhận diện các đồ vật xung quanh mình trong môi trường mới hoặc chưa quen thuộc.
\\
\textbf{Ràng buộc}: Người dùng cần giữ điện thoại hướng về phía khu vực có các đồ vật, không che camera, và đảm bảo độ sáng đủ để hệ thống nhận diện chính xác.
\\
\textbf{Các tình huống có thể xảy ra}:
\\
\begin{itemize}
    \item \textbf{Tình huống thành công}: Người dùng giơ điện thoại lên đúng hướng và hệ thống nhận diện thành công các đồ vật xung quanh, đồng thời đọc tên và vị trí của chúng giúp người dùng dễ dàng hình dung môi trường.
    \item \textbf{Tình huống không thành công}:
    \begin{itemize}
        \item \textbf{Không có tên đồ vật được đọc}: Nguyên nhân do các đồ vật trong khung hình không thuộc danh mục đã huấn luyện, hoặc không thể nhận diện do góc chụp khó.
    \end{itemize}
    \item \textbf{Tình huống ngoại lệ}:
    \begin{itemize}
        \item Người dùng không giữ điện thoại đúng hướng: 
        \begin{itemize}
            \item Nguyên nhân: Người dùng có thể không giơ điện thoại đúng hướng để có thể bao quát đồ vật.
            \item Hành động của hệ thống: Hệ thống nhắc nhở người dùng điều chỉnh góc máy, ví dụ: “Vui lòng giơ điện thoại hướng về phía đồ vật cần nhận diện.”
        \end{itemize}
        \item Kết nối gián đoạn: 
        \begin{itemize}
            \item Nguyên nhân: Mất kết nối với mô hình nhận diện đồ vật hoặc mô hình text to speech, gây gián đoạn việc phát thông tin về các đồ vật.
            \item Hành động của hệ thống: Hệ thống thông báo lỗi kết nối và yêu cầu người dùng thử lại sau.
        \end{itemize}
    \end{itemize}
\end{itemize}
\subsubsection{Chức năng 2: Khám phá môi trường xung quanh}
Chức năng khám phá môi trường xung quanh cho phép người dùng đặt câu hỏi về nội dung trong ảnh và nhận câu trả lời từ hệ thống, hỗ trợ người khiếm thị trong việc hiểu rõ hơn về môi trường xung quanh.
\\
\textbf{Người thực hiện}: Người dùng ứng dụng (đặc biệt là người khiếm thị hoặc những người gặp khó khăn trong nhận diện môi trường).
\\
\textbf{Mục tiêu}: Hỗ trợ người dùng khám phá và hiểu môi trường xung quanh bằng cách đặt câu hỏi về nội dung của bức ảnh mà họ cung cấp, thông qua mô tả âm thanh và hỏi-đáp dựa trên hình ảnh.
\\
\textbf{Tiền điều kiện}:
\begin{itemize}
    \item Người dùng đã mở ứng dụng và cấp quyền truy cập camera, microphone, và quyền xử lý hình ảnh.
    \item Hệ thống đã tích hợp mô hình PaliGemma (cho inference) và mô hình text-to-speech.
\end{itemize}
\\
\textbf{Luồng sự kiện chính}:
\\
\begin{longtable}{|p{6cm}|p{8cm}|}
\hline
\textbf{Hành động của người dùng} & \textbf{Hành động của hệ thống} \\
\hline
1. Người dùng khởi động ứng dụng và ứng dụng ở chế độ phát hiện vật thể (mặc định). & 
2. Ứng dụng mở và chuyển sang chế độ phát hiện đồ vật (chế độ mặc định). Hệ thống thông báo đã sẵn sàng qua âm thanh hoặc tín hiệu. \\
\hline
3. Người dùng ra lệnh chọn chế độ "Explore Destination" qua thao tác vuốt từ trái sang phải một lần. & 
4. Hệ thống khởi động camera và chuyển sang chế độ chờ người dùng chụp ảnh. \\
\hline
5. Người dùng chụp khung cảnh theo hướng mong muốn và chạm vào màn hình 2 lần để chụp ảnh. & 
6. Hệ thống nhận ảnh từ người dùng, lưu hình ảnh vào trong bộ nhớ cache. \\
\hline
7.1. Người dùng di chuyển tay vào phía dưới màn hình, bấm nút camera để chụp ảnh mới. & 
8.1. Hệ thống quay lại bước 4. \\
\hline
7.2. Người dùng click vào màn hình 2 lần để hệ thống nhận giọng nói. & 
8.2. Hệ thống chuyển sang chế độ speech-to-text của Google. \\
\hline
9. Người dùng đặt câu hỏi chi tiết (ví dụ: "Where is my laptop?"). & 
10. Hệ thống nhận câu hỏi từ người dùng, chuyển sang text bằng module speech-to-text. \\
\hline
& 11. Hệ thống gửi nội dung câu hỏi cùng ảnh tới server API của mô hình PaliGemma trên HuggingFace để trả về câu trả lời dưới dạng văn bản cho hệ thống. Trong thời gian gửi, hệ thống chặn không cho người dùng thao tác trong chế độ nhưng vẫn cho phép chuyển sang chế độ khác. \\
\hline
& 12. Hệ thống nhận được văn bản câu trả lời từ server và chuyển câu trả lời từ văn bản thành giọng nói và phát cho người dùng. \\
\hline
13. Người dùng nghe câu trả lời và có thể tiếp tục hỏi thêm hoặc chụp ảnh khác nếu cần. & 
14. Hệ thống sau khi phát xong câu trả lời thì quay lại trạng thái ở bước 6. \\
\hline
\end{longtable}
\\
\textbf{Tần suất sử dụng}: Chức năng này có thể được sử dụng thường xuyên, đặc biệt khi người dùng cần biết chi tiết về môi trường xung quanh hoặc có những câu hỏi cụ thể về các vật thể hoặc tình huống trong bức ảnh.
\\
\textbf{Ràng buộc}:
\begin{itemize}
    \item \textbf{Chất lượng ảnh}: Người dùng cần chụp ảnh rõ ràng, không mờ hoặc thiếu sáng. Nếu ảnh không đạt yêu cầu, hệ thống sẽ yêu cầu chụp lại.
    \item \textbf{Kết nối hệ thống}: Hệ thống phải có kết nối mạng để gửi yêu cầu tới server API của HuggingFace.
    \item \textbf{Khoảng cách thiết bị}: Người dùng cần giữ điện thoại đủ gần với môi trường để hệ thống có thể chụp được ảnh rõ ràng.
\end{itemize}
\\
\textbf{Các tình huống có thể xảy ra}:
\begin{itemize}
    \item \textbf{Tình huống thành công}: Người dùng chụp ảnh rõ nét. Hệ thống thực hiện inference thành công và phát ra mô tả về cảnh vật xung quanh bằng âm thanh. Người dùng có thể dễ dàng tiếp tục đặt câu hỏi về nội dung ảnh.
    \item \textbf{Tình huống không thành công}:
    \begin{itemize}
        \item \textbf{Inference không thành công}: Nguyên nhân: Mô hình PaliGemma không thể sinh mô tả do thông tin ảnh phức tạp hoặc không nhận diện được đối tượng.
        \item Hành động: Hệ thống thông báo sự cố và yêu cầu người dùng chụp lại hoặc thử lại sau.
    \end{itemize}
    \item \textbf{Tình huống ngoại lệ}:
    \begin{itemize}
        \item Người dùng không chụp đúng cảnh cần mô tả:
        \begin{itemize}
            \item Nguyên nhân: Người dùng có thể chụp không đúng hướng hoặc không bao quát môi trường xung quanh.
            \item Hành động: Hệ thống mô tả những gì có trong khung ảnh, nhưng nếu không đúng yêu cầu, người dùng có thể chụp lại.
        \end{itemize}
        \item Kết nối gián đoạn:
        \begin{itemize}
            \item Nguyên nhân: Mất kết nối mạng khiến quá trình gửi nhận yêu cầu tới server API của HuggingFace của mô hình PaliGemma, khiến không thể nhận được yêu cầu.
            \item Hành động: Hệ thống thông báo lỗi kết nối và yêu cầu người dùng kiểm tra lại mạng hoặc thử lại sau.
        \end{itemize}
    \end{itemize}
\end{itemize}
\\
\subsubsection{Chức năng 3: Nhận diện người quen}
Chức năng nhận diện người quen hỗ trợ người dùng tăng cường giao tiếp với những người xung quanh thông qua việc nhận diện và đọc tên những người được lưu trong thiết bị, giúp người dùng có thêm trải nghiệm giao tiếp xã hội.
\\
\textbf{Người thực hiện}: Người dùng ứng dụng (đặc biệt là người khiếm thị hoặc những người gặp khó khăn trong nhận diện môi trường).
\\
\textbf{Mục tiêu}: Hỗ trợ người dùng trong việc giao tiếp, bằng cách đưa điện thoại chạy ứng dụng lên và ứng dụng sẽ nhận diện, đọc tên những người được lưu tên và khuôn mặt trong thiết bị (có thể là người thân, người quen, người quan trọng).
\\
\textbf{Tiền điều kiện}:
\begin{itemize}
    \item Người dùng đã mở ứng dụng và cấp quyền truy cập camera, microphone, và quyền xử lý hình ảnh.
    \item Hệ thống đã tích hợp mô hình face detection (API Google ML Kit) và mô hình MobileFaceNet.
\end{itemize}
\\
\textbf{Luồng sự kiện chính}:
\\
\begin{longtable}{|p{6cm}|p{8cm}|}
\hline
\textbf{Hành động của người dùng} & \textbf{Hành động của hệ thống} \\
\hline
1. Người dùng ra lệnh chọn chế độ “Socializing Mode” thông qua thao tác vuốt theo trục Oy. & 
2. Hệ thống xác nhận yêu cầu của người dùng và thông báo chế độ "Face Recognition Mode" - chế độ mặc định của nhóm “Socializing Mode” đã được chọn thành công bằng âm thanh. Camera bật sẵn. \\
\hline
3. Người dùng giơ điện thoại lên, hướng camera đến khu vực muốn quét. &\\
\hline
4.1 Người dùng nhấn nút chụp người đối diện và đọc tên người đã được chụp để lưu, bấm hai nút để lưu vào. & \\
\hline
4.2 Người dùng chỉ đưa điện thoại lên phía trước để nhận dạng người quen. & 
5. Hệ thống sử dụng MLKit nhận diện, vẽ các bounding box cho các khuôn mặt trong khung hình và cho các bounding box qua MobileFaceNet xác định tên từng người. \\
\hline
& 6. Hệ thống chuyển các tên xác định được thành giọng nói và phát ra, nếu không xác định thì không phản hồi.\\
\hline
7. Người dùng nghe các tên / biệt danh về mọi người xung quanh và có thể nhận diện được đang có những ai xung quanh mình. &
8. Khi người dùng di chuyển camera hoặc phát hiện người mới, hệ thống quay lại trạng thái 2. \\
\hline
9. Người dùng có thể chuyển sang chế độ khác bằng cách vuốt sang trái hoặc sang phải, hoặc lên trên, hoặc xuống dưới. &
10. Hệ thống dừng chế độ nhận diện khuôn mặt và thông báo cho người dùng rằng chế độ đã kết thúc.
\\
\hline
\end{longtable}
\\
\textbf{Tần suất sử dụng}: Chức năng này có thể được sử dụng thường xuyên, đặc biệt khi người dùng cần biết về những người xung quanh mình, để có thể chủ động giao tiếp với người quen, hoặc làm quen người lạ, hoặc tránh xa những người xa lạ (đối với người dùng là trẻ em).
\\
\textbf{Ràng buộc}:
\begin{itemize}
    \item \textbf{Chất lượng ảnh}: Người dùng cần giữ điện thoại hướng về phía khu vực nghi ngờ có người xung quanh, không che camera, và đảm bảo độ sáng đủ để hệ thống nhận diện chính xác.
\end{itemize}
\\
\textbf{Các tình huống có thể xảy ra}:
\begin{itemize}
    \item \textbf{Tình huống thành công}: Người dùng giơ điện thoại lên đúng hướng và hệ thống nhận diện thành công mọi người xung quanh xuất hiện trong khung hình, đồng thời đọc tên của họ giúp người dùng dễ dàng biết có những ai xung quanh mình.
    \item \textbf{Tình huống không thành công}:
    \begin{itemize}
        \item \textbf{Không có tên người được đọc}: Mọi người trong khung hình không thuộc danh mục đã huấn luyện.
        \item \textbf{Không thể nhận diện do góc chụp không đủ thông tin}: Góc chụp có thể quá gần, quá xa, hoặc quá nghiêng để nhận diện chính xác khuôn mặt.
    \end{itemize}
    \item \textbf{Tình huống ngoại lệ}:
    \begin{itemize}
        \item \textbf{Người dùng không giữ điện thoại đúng hướng}: Người dùng có thể không giơ điện thoại đúng hướng để có thể bao quát thấy toàn bộ khuôn mặt.
        \item \textbf{Kết nối gián đoạn}: Mất quyền kết nối với camera, có thể do thiết bị của người dùng khởi động lại hoặc có chút trục trặc, cần cấp quyền kết nối lại với camera.
    \end{itemize}
\end{itemize}
\subsubsection{Chức năng 4: Theo dõi cảm xúc trong giao tiếp}
Chức năng theo dõi cảm xúc người đối diện hỗ trợ người dùng tăng cường giao tiếp với những người xung quanh thông qua giúp họ cảm nhận được cảm xúc của người đối diện mà họ đang giao tiếp.
\\
\textbf{Người thực hiện}: Người dùng ứng dụng.
\\
\textbf{Mục tiêu}: Hỗ trợ người dùng trong việc giao tiếp, bằng cách đưa điện thoại chạy ứng dụng lên và ứng dụng sẽ nhận diện cảm xúc của người đối diện, phát ra âm thanh vui tươi hoặc trầm ứng với cảm xúc buồn hoặc vui của đối phương.
\\
\textbf{Tiền điều kiện}:
\begin{itemize}
    \item Người dùng đã mở ứng dụng và cấp quyền truy cập camera, microphone, và quyền xử lý hình ảnh.
\end{itemize}

\textbf{Luồng sự kiện chính}:
\\
\begin{longtable}{|p{6cm}|p{8cm}|}
\hline
\textbf{Hành động của người dùng} & \textbf{Hành động của hệ thống} \\
\hline
1. Người dùng ra lệnh chọn chế độ “Socializing Mode” thông qua thao tác vuốt theo trục Oy. & 
2. Hệ thống xác nhận yêu cầu của người dùng và thông báo chế độ "Face Recognition Mode" - chế độ mặc định của nhóm “Socializing Mode” đã được chọn thành công bằng âm thanh. Camera bật sẵn. \\
\hline
3. Người dùng vuốt sang phải để chuyển sang chế độ “Mood tracking”. & 
4. Hệ thống xác nhận yêu cầu của người dùng và thông báo đã chuyển sang chế độ "Mood Tracking Mode". \\
\hline
5. Người dùng đưa điện thoại lên phía trước để nhận diện cảm xúc người đối diện trong cuộc trò chuyện. & 
6. Hệ thống sử dụng MLKit nhận diện cảm xúc của người đối diện. \\
\hline
& 7. Hệ thống sử dụng MLKit để phát ra âm thanh vui tươi nếu người đối diện vui vẻ (ví dụ như đang mỉm cười) và âm thanh trầm nếu người đối diện đang buồn. \\
\hline
7. Người dùng nghe âm thanh để biết được cảm xúc của người đối diện ra sao và từ đó điều chỉnh cách giao tiếp của mình cho phù hợp với không khí cuộc giao tiếp. &\\
\hline
8. Người dùng có thể chuyển sang chế độ khác bằng cách vuốt sang trái hoặc sang phải, hoặc lên trên, hoặc xuống dưới. & 
9. Hệ thống dừng chế độ mood tracking và thông báo cho người dùng đã chuyển sang chế độ khác. \\
\hline
\end{longtable}


\textbf{Tần suất sử dụng}: Chức năng này có thể được sử dụng thường xuyên, đặc biệt trong các cuộc trò chuyện, giao tiếp hàng ngày của người dùng khiếm thị.

\textbf{Ràng buộc}:
\begin{itemize}
    \item \textbf{Chất lượng ảnh}: Người dùng cần giữ điện thoại hướng về phía khu vực nghi ngờ có người xung quanh, không che camera, và đảm bảo độ sáng đủ để hệ thống nhận diện chính xác.
\end{itemize}

\textbf{Các tình huống có thể xảy ra}:

\begin{itemize}
    \item \textbf{Tình huống thành công}: Người dùng giơ điện thoại lên đúng hướng và hệ thống nhận diện chính xác cảm xúc của người đối diện, phát ra âm thanh ứng với cảm xúc đó.
    \item \textbf{Tình huống không thành công}:
    \begin{itemize}
        \item \textbf{Không phát ra âm thanh ứng với cảm xúc}: Nguyên nhân có thể là:
        \begin{itemize}
            \item Người dùng không hướng camera về phía một khuôn mặt nhất định.
            \item Người đối diện không có biểu cảm buồn hoặc vui mà có các biểu cảm khác như tức giận hoặc trung lập.
        \end{itemize}
        \item \textbf{Hành động của hệ thống}: Hệ thống không phản hồi.
    \end{itemize}
    \item \textbf{Tình huống ngoại lệ}:
    \begin{itemize}
        \item \textbf{Người dùng không giữ điện thoại đúng hướng}: Người dùng có thể không giơ điện thoại đúng hướng để có thể bao quát thấy toàn bộ khuôn mặt.
        \item \textbf{Hành động của hệ thống}: Hệ thống nhắc nhở người dùng điều chỉnh góc máy, ví dụ: “Vui lòng giơ điện thoại hướng về phía người bạn đang nói chuyện”.
        \item \textbf{Kết nối gián đoạn}: Mất kết nối với mô hình phát hiện khuôn mặt hoặc mô hình MobileFaceNets, gây gián đoạn việc phát thông tin về cảm xúc của người đối diện.
        \item \textbf{Hành động của hệ thống}: Hệ thống thông báo lỗi kết nối và yêu cầu người dùng kiểm tra khởi động lại ứng dụng hoặc thử lại sau.
    \end{itemize}
\end{itemize}

\subsection{Triển khai ứng dụng}
Nhóm phát triển ứng dụng trên Android studio với các công cụ đã đề cập ở phần 3.3 Công nghệ sử dụng. Các bước sử dụng mã nguồn như sau:
\begin{enumerate}
    \item Cài đặt Android Studio, Android SDK và Jetpack compose.
    \item Tải source code từ: \texttt{thebeo2004/INT2041\_HCI\_A-mobile-app-for-disabled-people}.
    \item Giải nén và mở thư mục trong Android Studio, cài đặt các package cần thiết trong file Gradle.
    \item Kết nối thiết bị di động Android với máy tính thông qua USB debugger.
    \item Lựa chọn thiết bị Android đã kết nối.
    \item Bấm nút chạy.
\end{enumerate}

Nhóm đã có bản file apk của ứng dụng, có thể tải xuống và chạy trên điện thoại hệ điều hành Android: \href{https://drive.google.com/file/d/1o4fchQpnmcFAMVoahwLiwrBWShIv1h_4/view?usp=sharing}{apk file}

\section{Kết quả}
\subsection{Kết quả đạt được}

Ứng dụng đã được triển khai thành công và có thể thực hiện các chức năng chính như xác định toàn bộ vật thể xung quanh, mô tả môi trường xung quanh, nhận diện người quen, và theo dõi cảm xúc trong giao tiếp. Các tính năng này đã được thử nghiệm với một số người tiêu dùng và họ đã khá hài lòng với các chức năng mà ứng dụng cung cấp. Các thử nghiệm cho thấy ứng dụng có khả năng phục vụ nhu cầu của người khiếm thị một cách hiệu quả, đặc biệt trong việc hỗ trợ nhận diện môi trường và giao tiếp xã hội. Tuy nhiên, để nâng cao trải nghiệm người dùng, các chức năng này vẫn cần được tối ưu hóa về mặt tốc độ và độ chính xác.

\subsection{So sánh với các ứng dụng hiện tại}
Ứng dụng của nhóm hơn các ứng dụng object detection như Seeing AI khi không chỉ xác định các vật thể mà còn cung cấp vị trí chi tiết (trái, phải, trước, sau) và đọc tên các vật thể một cách lần lượt, giúp người dùng hình dung chính xác không gian xung quanh. Bên cạnh đó, so với các sản phẩm trước đó, ứng dụng có tính năng Explore, cho phép người dùng hỏi đáp về hình ảnh môi trường xung quanh bằng sự kết hợp giữa ngôn ngữ và hình ảnh (multimodal), áp dụng công nghệ GenAI hiện đại. Tính năng này không chỉ cung cấp thông tin mô tả môi trường mà còn đáp ứng linh hoạt các câu hỏi của người dùng về không gian xung quanh, mang lại trải nghiệm cá nhân hóa và tương tác tự nhiên hơn. Cuối cùng, giao diện của ứng dụng được thiết kế thân thiện với người khiếm thị, cho phép họ sử dụng dễ dàng qua các thao tác chạm, vuốt và nói. Điều này khắc phục nhược điểm về giao diện khó sử dụng của các ứng dụng hiện nay, mang lại trải nghiệm thuận tiện và tự nhiên hơn.

\section{Thảo luận}

\subsection{Những khó khăn trong quá trình triển khai}

Trong quá trình phát triển ứng dụng, nhóm đã gặp phải một số khó khăn lớn, đặc biệt do đây là lần đầu các thành viên thực hiện dự án phát triển ứng dụng di động. Điều này khiến cả nhóm phải dành nhiều thời gian tìm hiểu các công nghệ mobile khác nhau nhằm chọn ra công nghệ phù hợp nhất với nhóm.

Thêm vào đó, bài toán image captioning gặp nhiều thử thách kỹ thuật. Ban đầu, nhóm dự kiến triển khai image captioning theo hướng thời gian thực, tuy nhiên phương pháp này nảy sinh vấn đề: việc mô tả ảnh thay đổi liên tục theo hướng di chuyển của điện thoại gây khó khăn khi chuyển đổi câu caption sang giọng nói qua module text-to-speech. Sau khi đánh giá, nhóm đã chuyển sang cách chụp ảnh và hỏi-đáp với các mô hình sinh văn bản. Giải pháp này không chỉ giải quyết được vấn đề ổn định mô tả ảnh mà còn giúp người dùng có thể đặt nhiều câu hỏi khác nhau về ảnh chụp, từ đó cung cấp thông tin chi tiết hơn so với chỉ một câu mô tả ngắn của các mô hình image caption (như CLIP, Cam2Text,...) thông thường.

Ứng dụng hiện tại gặp một số hạn chế chính. Thứ nhất, độ trễ khi gọi API là vấn đề đáng chú ý, đặc biệt khi sử dụng Retrofit và OkHttp để gọi API từ Hugging Face, điều này ảnh hưởng đến trải nghiệm người dùng khi cần phản hồi nhanh chóng. Thứ hai, phạm vi nhận diện vật thể của hệ thống còn hạn hẹp, hiện tại chỉ tập trung vào một số vật thể thông dụng, điều này hạn chế khả năng ứng dụng trong môi trường thực tế đa dạng và phức tạp. Các hạn chế này cần được cải thiện trong tương lai để nâng cao hiệu quả và khả năng sử dụng của ứng dụng.

\subsection{Định hướng phát triển}

Để khắc phục các hạn chế hiện tại, nhóm sẽ tập trung vào việc tối ưu hóa độ trễ khi gọi API, nghiên cứu và triển khai các mô hình nhận diện vật thể đa dạng hơn để mở rộng phạm vi nhận diện của hệ thống. Đồng thời, nhóm cũng sẽ cải tiến các tính năng của ứng dụng để đáp ứng nhu cầu người dùng một cách tốt hơn, đặc biệt là người khiếm thị, giúp họ có thể hòa nhập xã hội dễ dàng hơn và tăng cường sự độc lập trong cuộc sống hàng ngày.

\section{Kết luận}

Dự án phát triển ứng dụng di động hỗ trợ người khiếm thị đã đạt được những kết quả tích cực, với các chức năng chính như hỗ trợ di chuyển an toàn, nhận biết môi trường xung quanh, và giao tiếp xã hội. Ứng dụng đã được thử nghiệm trong môi trường thực tế, bước đầu cho thấy tiềm năng cải thiện chất lượng cuộc sống của người khiếm thị thông qua việc tăng cường tính độc lập và khả năng hòa nhập xã hội. Tuy nhiên, ứng dụng vẫn còn một số hạn chế, như độ trễ khi gọi API, phạm vi nhận diện vật thể còn hạn hẹp. Những khó khăn này đã được nhóm nhận diện và đề xuất các định hướng phát triển trong tương lai nhằm khắc phục và hoàn thiện sản phẩm, từ đó mở rộng phạm vi ứng dụng và nâng cao trải nghiệm người dùng. Với nền tảng đã xây dựng và những định hướng rõ ràng, nhóm tin rằng ứng dụng có thể trở thành một công cụ hữu ích, không chỉ hỗ trợ người khiếm thị trong cuộc sống hằng ngày mà còn mở ra cơ hội ứng dụng rộng rãi trong nhiều lĩnh vực khác.

\section{Phân chia công việc}

Để đảm bảo sự tiến triển của dự án, công việc được phân chia rõ ràng giữa các thành viên trong nhóm. Các nhiệm vụ chính bao gồm:

\renewcommand{\arraystretch}{1.5} % Tăng khoảng cách dòng trong bảng

\setlength{\LTleft}{0pt}
\setlength{\LTright}{0pt}

\small
\begin{longtable}{|l|l|l|p{6cm}|}
\hline
\textbf{Họ và tên}           & \textbf{MSSV} & \textbf{Đóng góp} & \textbf{Công việc}                                                                                                     \\ \hline
Tăng Vĩnh Hà (Nhóm trưởng) & 22028129      & 20\%              & 
Lên ý tưởng, tìm hiểu công nghệ triển khai chung để làm ứng dụng. \newline
Tìm kiếm các công nghệ liên quan chức năng 2. \newline
Thực hiện cài đặt chức năng 2. \newline
Làm báo cáo, slide, thuyết trình, quay video demo sản phẩm. \newline
Triển khai mô hình vào chức năng nhận diện vật thể. \\ \hline

Nguyễn Hữu Thế              & 22028155      & 20\%              & 
Tìm kiếm các công nghệ liên quan chức năng 3 và 4. \newline
Thực hiện cài đặt chức năng 3 và chức năng 4. \newline
Thuyết trình. \\ \hline

Vũ Thị Minh Thư           & 22028116      & 20\%              & 
Tìm kiếm các công nghệ liên quan chức năng 1. \newline
Thực hiện cài đặt chức năng 1. \newline
Làm tài liệu, slide, quay video demo. \\ \hline

Chu Quang Cần             & 22028093      & 20\%              & 
Tìm kiếm các công nghệ liên quan chức năng 1. \newline
Thực hiện cài đặt chức năng 1. \newline
Làm tài liệu, slide, quay video demo. \\ \hline

Lê Xuân Hùng             & 22028172      & 20\%              & 
Tìm kiếm các công nghệ liên quan chức năng 2. \newline
Thực hiện cài đặt chức năng 2. \newline
Làm tài liệu, slide, quay video demo. \\ \hline

\caption{Phân chia công việc trong nhóm}
\label{table:phan_chia_cong_viec}
\end{longtable}



Các thành viên trong nhóm cũng đã cùng nhau phối hợp để giải quyết những vấn đề phát sinh trong quá trình triển khai, đảm bảo tiến độ và chất lượng của dự án.











% \section{Đề xuất giải pháp}

% \subsection{Khảo sát các sản phẩm đã có trên thị trường}
% \begin{itemize}
%     \item \textbf{Seeing AI:} Do Microsoft phát triển, ứng dụng này sử dụng trí tuệ nhân tạo để mô tả môi trường xung quanh, đọc văn bản, nhận diện khuôn mặt và vật thể thông qua giọng nói.



%     \item \textbf{KNFB Reader:} Ứng dụng này cho phép người dùng chụp ảnh tài liệu và chuyển đổi chúng thành văn bản âm thanh hoặc văn bản kỹ thuật số, hỗ trợ việc đọc nhanh chóng.



%     \item \textbf{Voice Dream Reader:} Ứng dụng đọc văn bản đa dạng từ sách, tài liệu đến email, hỗ trợ người khiếm thị tiếp cận thông tin dễ dàng hơn.


% \end{itemize}

% \subsection{Đối tượng sử dụng}
% Ứng dụng được thiết kế dành riêng cho người khiếm thị – những người gặp khó khăn trong việc nhận diện và hiểu biết về không gian xung quanh. Bằng cách cung cấp các tính năng nhận diện vật cản nguy hiểm, xác định đồ vật, và mô tả môi trường xung quanh, ứng dụng sẽ giúp người dùng nắm bắt thông tin cần thiết, tăng cường an toàn khi di chuyển và nâng cao khả năng tương tác trong các hoạt động hàng ngày.

% \subsection{Chức năng 1: Cảnh báo nguy hiểm cho người dùng khi đi đường}
% Object detection là bài toán trong lĩnh vực thị giác máy nhằm xác định vị trí và phân loại các đối tượng trong hình ảnh hoặc video. Đối với ứng dụng di động hỗ trợ người khiếm thị của nhóm, object detection giúp nhận diện các vật thể và chướng ngại vật trong môi trường xung quanh người dùng khiếm thị, giúp họ có thể di chuyển an toàn hơn. Input của mô hình là hình ảnh hoặc khung hình từ camera của điện thoại. Output là tên và vị trí của các vật thể được nhận diện, được biểu diễn qua bounding box trên màn hình và tên đồ vật được mô tả bằng giọng nói.

% Chức năng đầu tiên của ứng dụng là cảnh báo người dùng về các vật cản nguy hiểm trên đường đi nhằm tăng cường an toàn trong quá trình di chuyển. Nhóm sử dụng mô hình đã được huấn luyện sẵn MobileNet để phát hiện các vật nguy hiểm trên đường, như ổ chuột, xe ô tô và xe máy tiến gần đến người dùng. MobileNet được chọn vì có khả năng phát hiện vật thể nhanh, độ chính xác cao, kích thước nhỏ gọn và tốc độ suy luận nhanh, rất phù hợp cho các thiết bị di động.

% Sau khi phát hiện vật thể nguy hiểm, hệ thống sẽ kích hoạt module chuyển văn bản thành giọng nói (text-to-speech) để đọc tên và vị trí tương đối của vật thể so với người dùng thông qua kênh âm thanh. Đồng thời, ứng dụng sẽ bật chế độ rung để cảnh báo đến khi vật thể nguy hiểm không còn xuất hiện trong khung hình camera. Đây là tính năng mặc định, luôn tự động bật khi người dùng khởi động ứng dụng, đảm bảo rằng người khiếm thị có thể nhanh chóng nhận diện và tránh các nguy hiểm tiềm ẩn trong môi trường xung quanh.


% \subsection{Chức năng 2: Xác định toàn bộ vật thể xung quanh}
% Chức năng thứ hai của ứng dụng là xác định các đồ vật xung quanh người dùng, cung cấp thông tin chi tiết về môi trường xung quanh nhằm hỗ trợ khả năng tương tác trong các hoạt động hàng ngày. Nhóm tiếp tục sử dụng mô hình MobileNet để duy trì kích thước gọn nhẹ và tốc độ xử lý nhanh, phù hợp cho thiết bị di động.

% Không giống chức năng 1 chỉ tập trung vào các vật thể nguy hiểm, chức năng 2 mở rộng phạm vi nhận diện ra toàn bộ các đồ vật có thể thấy, dựa trên danh sách nhãn đã được huấn luyện trong mô hình. Quan trọng hơn, chức năng 2 còn có thể mô tả cho người dùng được vị trí của các đồ vật (thay vì mỗi đọc tên thì hệ thống sẽ còn phát ra âm thanh mô tả như “bottle on the right”, “chair in the front”). Sau khi xác định được các đồ vật xung quanh, ứng dụng sẽ sử dụng module chuyển văn bản thành giọng nói (text-to-speech) để đọc tên từng vật thể, giúp người dùng có thể hình dung và nắm bắt được thông tin chi tiết về không gian xung quanh họ.

% \subsection{Chức năng 3: Mô tả môi trường xung quanh}
% Visual Question Answering (VQA) là bài toán trong lĩnh vực trí tuệ nhân tạo, cho phép hệ thống trả lời các câu hỏi dựa trên nội dung của hình ảnh. Mô hình VQA nhận đầu vào là một bức ảnh và một câu hỏi dạng văn bản, sau đó phân tích để đưa ra câu trả lời phù hợp. Các mô hình giải quyết bài toán Visual Question Answering thường có kiến trúc kết hợp giữa các thành phần xử lý hình ảnh và ngôn ngữ. Ví dụ, Pali Gemma được xây dựng dựa trên hai thành phần chính: mô hình thị giác SigLIP và mô hình ngôn ngữ Gemma 2B. Cụ thể, SigLIP là một phần mã hóa hình ảnh sử dụng Vision Transformer để trích xuất đặc trưng hình ảnh, trong khi Gemma 2B là một mô hình Transformer đóng vai trò như bộ giải mã ngôn ngữ. Pali Gemma nhận đầu vào là hình ảnh và văn bản, sau đó tạo ra văn bản đầu ra, phù hợp cho các tác vụ như mô tả và trả lời câu hỏi về hình ảnh. Trong ứng dụng của nhóm, VQA giúp người dùng có thể đặt câu hỏi về các chi tiết cụ thể của môi trường xung quanh, tăng khả năng tương tác và hiểu biết về không gian họ đang ở.

% Chức năng thứ ba của ứng dụng là sinh ra một câu mô tả ngắn gọn về môi trường xung quanh người dùng và cho phép họ hỏi đáp về môi trường xung quanh trông ra sao. Người dùng sẽ tiến hành chụp ảnh và nhóm sẽ sử dụng mô hình PaliGemma, một mô hình vision-language của Google, để thực hiện image captioning (mô tả bức ảnh một cách ngắn gọn) và hỗ trợ hỏi-đáp dựa trên nội dung hình ảnh đầu vào. Input mô hình sẽ là bức ảnh và câu prompt của người dùng (hay còn nói cách khác là câu hỏi, khi người dùng hỏi thì câu prompt có thể sẽ là “Describe the picture”). Tiếp đến, người dùng có thể tiếp tục hỏi về môi trường xung quanh. nhóm sẽ sử dụng mô hình đã được train thông qua API của Hugging Face, đồng thời sử dụng 2 module text to speech để chuyển câu trả lời text của mô hình sang âm thanh, speech to text để chuyển câu hỏi người dùng sang text cho input của mô hình.




% \section{Phân tích và đặc tả yêu cầu}

% \subsection{Chức năng 1: Phát hiện chế độ nguy hiểm}
% Chức năng cảnh báo chướng ngại vật giúp người dùng, đặc biệt là người khiếm thị hoặc người có khó khăn về di chuyển, tránh được các vật thể nguy hiểm trên đường đi bằng các cảnh báo kịp thời.

% \textbf{Người thực hiện:} Người dùng khi sử dụng ứng dụng.

% \textbf{Mục tiêu:} Giúp người dùng tránh các chướng ngại vật hoặc khu vực nguy hiểm trên đường đi.

% \textbf{Tiền điều kiện:}
% \begin{itemize}
%     \item Người dùng mở ứng dụng và đã cấp đầy đủ quyền (truy cập camera, microphone, cảm biến, định vị, v.v.).
%     \item Người dùng chọn chế độ cảnh báo chướng ngại vật bằng phím tắt (vuốt) hoặc giọng nói.
% \end{itemize}

% \textbf{Luồng sự kiện chính:}

% \begin{table}[H]
% \centering
% \begin{tabular}{|p{0.45\linewidth}|p{0.45\linewidth}|}
% \hline
% \textbf{Hành động của người dùng} & \textbf{Hành động của hệ thống} \\
% \hline
% 1. Người dùng khởi động ứng dụng bằng giọng nói hoặc giao diện đơn giản. & 2. Hệ thống xác nhận yêu cầu của người dùng và thông báo chế độ Cảnh báo nguy hiểm đã được chọn thành công bằng âm thanh hoặc tín hiệu rung. \\
% \hline
% 3. Người dùng bắt đầu di chuyển và cầm điện thoại ở phía trước để quét khu vực phía trước (camera hướng về phía đường đi). & 4. Hệ thống kích hoạt camera, cảm biến khoảng cách (nếu có) và bắt đầu quét môi trường xung quanh liên tục, đồng thời phát ra âm thanh hoặc rung nhẹ để xác nhận quá trình quét đang diễn ra. \\
% \hline
% & 5. Hệ thống phát hiện một chướng ngại vật nguy hiểm phía trước (ví dụ: một bậc thang, xe đẩy...). \\
% \hline
% & 6. Hệ thống lập tức phát cảnh báo bằng âm thanh (âm thanh to, liên tục) (ví dụ: “a ladder on the left”, “trolley in the front”...) và rung (nếu cần) để người dùng nhận biết sự nguy hiểm. \\
% \hline
% 7. Người dùng nhận thức được nguy hiểm và thay đổi hướng di chuyển. & 8. Khi người dùng đã đi qua hoặc tránh được chướng ngại vật, hệ thống dừng cảnh báo. \\
% \hline
% & 9. Hệ thống thông báo rằng người dùng đã thoát khỏi khu vực nguy hiểm, có thể bằng âm thanh hoặc tín hiệu rung để xác nhận sự an toàn. \\
% \hline
% 10. Người dùng tiếp tục di chuyển & 11. Hệ thống vẫn tiếp tục quét và cảnh báo khi phát hiện chướng ngại vật khác (nếu có). \\
% \hline
% \end{tabular}
% \caption{Luồng sự kiện chính giữa hành động của người dùng và hành động của hệ thống.}
% \end{table}

% \textbf{Tần suất sử dụng:} Thường xuyên, bất cứ khi nào người dùng di chuyển trong môi trường có thể xuất hiện chướng ngại vật.

% \textbf{Ràng buộc:}
% \begin{itemize}
%     \item Người dùng phải giữ điện thoại hướng về phía trước, không che camera hoặc cảm biến, để hệ thống có thể quét môi trường chính xác.
%     \item Người dùng cần nói to, rõ ràng và cách thiết bị không quá xa (khoảng cách dưới 1m) để hệ thống có thể nhận diện giọng nói chính xác.
% \end{itemize}

% \textbf{Các tình huống có thể xảy ra:}
% \begin{itemize}
%     \item \textbf{Tình huống thành công:} Người dùng nhận được cảnh báo đúng thời điểm khi gặp chướng ngại vật. Sau đó, họ điều chỉnh hướng di chuyển và hệ thống ngừng cảnh báo khi người dùng an toàn.
%     \item \textbf{Tình huống không thành công:}
%     \begin{itemize}
%         \item Không phát hiện chướng ngại vật:
%             \begin{itemize}
%                 \item Nguyên nhân: Do ánh sáng quá yếu hoặc camera bị che khuất.
%             \end{itemize}
%         \item Không có cảnh báo kịp thời:
%             \begin{itemize}
%                 \item Nguyên nhân: Kết nối bị gián đoạn, hoặc chướng ngại vật xuất hiện đột ngột.
%                 \item Hành động của hệ thống: Hệ thống sẽ tự khởi động lại quá trình quét khi phát hiện sự cố.
%             \end{itemize}
%         \item Người dùng không nghe rõ cảnh báo:
%             \begin{itemize}
%                 \item Nguyên nhân: Âm thanh cảnh báo quá nhỏ hoặc môi trường quá ồn.
%                 \item Hành động của hệ thống: Cảnh báo qua rung mạnh hơn hoặc đưa ra thông báo bằng tín hiệu đèn sáng (nếu có). Nếu người dùng bỏ qua cảnh báo, hệ thống có thể ghi nhận sự cố này để điều chỉnh âm thanh cho các lần sử dụng sau.
%             \end{itemize}
%     \end{itemize}
%     \item \textbf{Tình huống ngoại lệ:} 
%     \begin{itemize}
%         \item Người dùng không thực hiện đúng: Người dùng có thể không cầm điện thoại đúng cách, dẫn đến việc quét không chính xác.
%         \item Hành động của hệ thống: Hệ thống sẽ cảnh báo người dùng điều chỉnh vị trí điện thoại.
%     \end{itemize}
% \end{itemize}

% \subsection{Chức năng 2: Xác định toàn bộ vật thể xung quanh}
% Chức năng phát hiện đồ vật giúp người dùng nhận diện các đồ vật xung quanh bằng cách xác định và đọc tên từng đồ vật trong khung hình camera, hỗ trợ đặc biệt cho người khiếm thị trong việc nhận biết môi trường.

% \textbf{Người thực hiện:} Người dùng ứng dụng (đặc biệt là người khiếm thị hoặc những người gặp khó khăn trong việc nhận diện môi trường).

% \textbf{Mục tiêu:} Giúp người dùng nhận diện và hiểu rõ các đồ vật xung quanh trong khung hình camera bằng cách đọc tên từng đồ vật kèm vị trí của chúng.

% \textbf{Tiền điều kiện:}
% \begin{itemize}
%     \item Người dùng mở ứng dụng và đã cấp đầy đủ quyền (truy cập camera, microphone, cảm biến, v.v.).
%     \item Người dùng chọn chế độ phát hiện đồ vật thông qua phím tắt hoặc giọng nói.
% \end{itemize}

% \textbf{Luồng Sự Kiện Chính:}

% \begin{table}[H]
% \centering
% \begin{tabular}{|p{0.45\linewidth}|p{0.45\linewidth}|}
% \hline
% \textbf{Hành động của Người dùng} & \textbf{Hành động của Hệ thống} \\
% \hline
% 1. Người dùng khởi động ứng dụng bằng giọng nói hoặc giao diện đơn giản. & 2. Ứng dụng mở và chuyển sang chế độ Cảnh báo nguy hiểm (chế độ mặc định). Hệ thống thông báo đã sẵn sàng qua âm thanh hoặc tín hiệu. \\
% \hline
% 3. Người dùng ra lệnh chọn chế độ phát hiện đồ vật qua thao tác vuốt. & 4. Hệ thống xác nhận yêu cầu của người dùng và thông báo chế độ đã được chọn thành công bằng âm thanh. \\
% \hline
% 5. Người dùng giơ điện thoại lên, hướng camera đến khu vực muốn quét để phát hiện đồ vật. & 6. Hệ thống kích hoạt camera và bắt đầu quét môi trường xung quanh, liên tục tìm kiếm các đồ vật trong khung hình. \\
% \hline
% & 7. Hệ thống nhận diện các đồ vật trong khung hình, vẽ các bounding box xung quanh chúng và xác định tên từng đồ vật (nếu nhận diện được). \\
% \hline
% & 8. Hệ thống lần lượt đọc tên các đồ vật đã nhận diện cùng với vị trí của chúng trong khung hình (ví dụ: "A chair in front", "A table on the right"). \\
% \hline
% 9. Người dùng nghe các mô tả về các đồ vật và có thể nhận diện được môi trường xung quanh mình. & 10. Khi người dùng di chuyển camera hoặc phát hiện đồ vật mới, hệ thống tiếp tục cập nhật và mô tả đồ vật mới trong khung hình. \\
% \hline
% 11. Người dùng có thể yêu cầu dừng chế độ phát hiện đồ vật bất kỳ lúc nào. & 12. Hệ thống dừng chế độ phát hiện đồ vật và thông báo cho người dùng rằng chế độ đã kết thúc. \\
% \hline
% \end{tabular}
% \caption{Luồng sự kiện chính giữa hành động của người dùng và hành động của hệ thống trong chế độ phát hiện đồ vật.}
% \end{table}

% \textbf{Tần suất Sử dụng:} Chức năng này thường được sử dụng khi người dùng muốn nhận diện các đồ vật xung quanh mình trong môi trường mới hoặc chưa quen thuộc.

% \textbf{Ràng buộc:}
% \begin{itemize}
%     \item Chất lượng ảnh: Người dùng cần giữ điện thoại hướng về phía khu vực có các đồ vật, không che camera, và đảm bảo độ sáng đủ để hệ thống nhận diện chính xác.
%     \item Kết nối hệ thống: Hệ thống phải có khả năng nhận diện đồ vật trong thời gian thực và phát ra thông tin bằng giọng nói tự nhiên.
% \end{itemize}

% \textbf{Các Tình Huống Có thể Xảy ra:}
% \begin{itemize}
%     \item \textbf{Tình huống Thành công:} Người dùng giơ điện thoại lên đúng hướng và hệ thống nhận diện thành công các đồ vật xung quanh, đồng thời đọc tên và vị trí của chúng giúp người dùng dễ dàng hình dung môi trường.
%     \item \textbf{Tình huống Không thành công:}
%     \begin{itemize}
%         \item Không có tên đồ vật được đọc:
%             \begin{itemize}
%                 \item Nguyên nhân: Các đồ vật trong khung hình không thuộc danh mục đã huấn luyện, hoặc không thể nhận diện do góc chụp khó.
%                 \item Hành động của hệ thống: Hệ thống thông báo về tình trạng không nhận diện được, và khuyến nghị người dùng di chuyển camera hoặc chọn góc khác.
%             \end{itemize}
%     \end{itemize}
%     \item \textbf{Tình huống Ngoại lệ:}
%     \begin{itemize}
%         \item Người dùng không giữ điện thoại đúng hướng:
%             \begin{itemize}
%                 \item Nguyên nhân: Người dùng có thể không giơ điện thoại đúng hướng để có thể bao quát đồ vật.
%                 \item Hành động của hệ thống: Hệ thống nhắc nhở người dùng điều chỉnh góc máy, ví dụ: “Vui lòng giơ điện thoại hướng về phía đồ vật cần nhận diện.”
%             \end{itemize}
%         \item Kết nối gián đoạn:
%             \begin{itemize}
%                 \item Nguyên nhân: Mất kết nối với mô hình nhận diện đồ vật hoặc mô hình Text to Speech (TTS), gây gián đoạn việc phát thông tin về các đồ vật.
%                 \item Hành động của hệ thống: Hệ thống thông báo lỗi kết nối và yêu cầu người dùng kiểm tra lại mạng hoặc thử lại sau.
%             \end{itemize}
%     \end{itemize}
% \end{itemize}

% \subsection{Chức năng 3: Khám phá môi trường xung quanh}
% Chức năng khám phá môi trường xung quanh cho phép người dùng đặt câu hỏi về nội dung trong ảnh và nhận câu trả lời từ hệ thống, hỗ trợ người khiếm thị trong việc hiểu rõ hơn về môi trường xung quanh.

% \textbf{Người thực hiện:} Người dùng ứng dụng (đặc biệt là người khiếm thị hoặc những người gặp khó khăn trong nhận diện môi trường).

% \textbf{Mục tiêu:} Hỗ trợ người dùng khám phá và hiểu môi trường xung quanh bằng cách đặt câu hỏi về nội dung của bức ảnh mà họ cung cấp, thông qua mô tả âm thanh và hỏi-đáp dựa trên hình ảnh.

% \textbf{Tiền điều kiện:}
% \begin{itemize}
%     \item Người dùng đã mở ứng dụng và cấp quyền truy cập camera, microphone, và quyền xử lý hình ảnh.
%     \item Hệ thống đã tích hợp mô hình PaliGemma (cho inference) và mô hình text-to-speech.
% \end{itemize}

% \textbf{Luồng Sự Kiện Chính:}

% \begin{table}[H]
% \centering
% \begin{tabular}{|p{0.45\linewidth}|p{0.45\linewidth}|}
% \hline
% \textbf{Hành động của Người dùng} & \textbf{Hành động của Hệ thống} \\
% \hline
% 1. Người dùng ra lệnh chọn chế độ "Explore Destination" qua thao tác vuốt. & 2. Hệ thống khởi động camera và chuyển sang chế độ chờ người dùng chụp ảnh. \\
% \hline
% 3. Người dùng chụp khung cảnh theo hướng mong muốn và nhấn nút chụp. & 4. Hệ thống nhận ảnh từ người dùng. \\
% \hline
% 5. Người dùng đặt câu hỏi chi tiết (ví dụ: "Where is my laptop?"). & 6. Hệ thống nhận câu hỏi từ người dùng (speech-to-text), sau đó gửi nội dung câu hỏi cùng ảnh tới mô hình PaliGemma để xử lý hỏi-đáp. \\
% \hline
% & 7. Hệ thống chuyển câu trả lời từ văn bản thành giọng nói và phát cho người dùng. \\
% \hline
% 8. Người dùng nghe câu trả lời và có thể tiếp tục hỏi thêm hoặc chụp ảnh khác nếu cần. & 9. Hệ thống trở về trạng thái chờ hoặc chuyển sang chế độ khác theo yêu cầu của người dùng. \\
% \hline
% \end{tabular}
% \caption{Luồng sự kiện chính giữa hành động của người dùng và hành động của hệ thống trong chế độ khám phá môi trường.}
% \end{table}

% \textbf{Tần suất Sử dụng:} Chức năng này có thể được sử dụng thường xuyên, đặc biệt khi người dùng cần biết chi tiết về môi trường xung quanh hoặc có những câu hỏi cụ thể về các vật thể hoặc tình huống trong bức ảnh.

% \textbf{Ràng buộc:}
% \begin{itemize}
%     \item Chất lượng ảnh: Người dùng cần chụp ảnh rõ ràng, không mờ hoặc thiếu sáng. Nếu ảnh không đạt yêu cầu, hệ thống sẽ yêu cầu chụp lại.
%     \item Kết nối hệ thống: Hệ thống phải có khả năng inference ảnh nhanh chóng và chính xác qua mô hình PaliGemma, đồng thời chuyển đổi văn bản sang giọng nói tự nhiên.
%     \item Khoảng cách thiết bị: Người dùng cần giữ điện thoại đủ gần với môi trường để hệ thống có thể chụp được ảnh rõ ràng.
% \end{itemize}

% \textbf{Các Tình Huống Có thể Xảy ra:}
% \begin{itemize}
%     \item \textbf{Tình huống Thành công:} Người dùng chụp ảnh rõ nét. Hệ thống thực hiện inference thành công và phát ra mô tả về cảnh vật xung quanh bằng âm thanh. Người dùng có thể dễ dàng tiếp tục đặt câu hỏi về nội dung ảnh.
%     \item \textbf{Tình huống Không thành công:} Inference không thành công:
%         \begin{itemize}
%             \item Nguyên nhân: Mô hình PaliGemma không thể sinh mô tả do thông tin ảnh phức tạp hoặc không nhận diện được đối tượng.
%             \item Hành động: Hệ thống thông báo sự cố và yêu cầu người dùng chụp lại hoặc thử lại sau.
%         \end{itemize}
%     \item \textbf{Tình huống Ngoại lệ:} Người dùng không chụp đúng cảnh cần mô tả:
%         \begin{itemize}
%             \item Nguyên nhân: Người dùng có thể chụp không đúng hướng hoặc không bao quát môi trường xung quanh.
%             \item Hành động: Hệ thống mô tả những gì có trong khung ảnh, nhưng nếu không đúng yêu cầu, người dùng có thể chụp lại.
%         \end{itemize}
%         \item Kết nối gián đoạn:
%             \begin{itemize}
%                 \item Nguyên nhân: Mất kết nối với mô hình PaliGemma hoặc mô hình TTS, gây lỗi trong xử lý ảnh hoặc phát âm thanh.
%                 \item Hành động: Hệ thống thông báo lỗi kết nối và yêu cầu người dùng kiểm tra lại mạng hoặc thử lại sau.
%             \end{itemize}
% \end{itemize}

% \section{Những khó khăn trong quá trình triển khai}

% Trong quá trình phát triển ứng dụng, nhóm đã gặp phải một số khó khăn lớn, đặc biệt do đây là lần đầu các thành viên thực hiện dự án phát triển ứng dụng di động. Điều này khiến cả nhóm phải dành nhiều thời gian tìm hiểu các công nghệ mobile khác nhau nhằm chọn ra công nghệ phù hợp nhất với nhóm.

% Thêm vào đó, bài toán image captioning gặp nhiều thử thách kỹ thuật. Ban đầu, nhóm dự kiến triển khai image captioning theo hướng thời gian thực, tuy nhiên phương pháp này nảy sinh vấn đề: việc mô tả ảnh thay đổi liên tục theo hướng di chuyển của điện thoại gây khó khăn khi chuyển đổi câu caption sang giọng nói qua module text-to-speech. Sau khi đánh giá, nhóm đã chuyển sang cách chụp ảnh và hỏi-đáp với các mô hình sinh văn bản. Giải pháp này không chỉ giải quyết được vấn đề ổn định mô tả ảnh mà còn giúp người dùng có thể đặt nhiều câu hỏi khác nhau về ảnh chụp, từ đó cung cấp thông tin chi tiết hơn so với chỉ một câu mô tả ngắn của các mô hình image caption (như CLIP, Cam2Text,...) thông thường.

% \section{Tiến độ dự án}

% Dự án hiện đang được phát triển theo các giai đoạn, với tiến độ như sau:

% \begin{itemize}
%     \item Đã hoàn thành việc triển khai (implementation) các chức năng cơ bản 1 và 2.
%     \item Đang triển khai chức năng 3.
%     \item Đang thực hiện tích hợp thống nhất cả ba chức năng vào một ứng dụng duy nhất.
%     \item Đang trong giai đoạn kiểm thử hệ thống để đảm bảo hoạt động ổn định, đồng thời tiến hành thử nghiệm thực tế nhằm đánh giá hiệu quả ứng dụng.
%     \item Đang phát triển một số tính năng phụ nhằm bổ trợ và nâng cao trải nghiệm của các chức năng chính (1, 2 và 3).
% \end{itemize}

% \section{Kế hoạch phát triển tiếp theo}

% Với những tiến bộ đã đạt được, nhóm đã đặt ra một số mục tiêu tiếp theo để hoàn thiện ứng dụng, bao gồm:

% \begin{itemize}
%     \item Hoàn thiện và tối ưu hóa chức năng hỏi-đáp dựa trên hình ảnh trong chức năng 3, đảm bảo người dùng có thể dễ dàng nhận được câu trả lời chính xác và nhanh chóng cho các câu hỏi về môi trường xung quanh.
%     \item Cải thiện độ chính xác và tốc độ xử lý của module nhận diện đồ vật và module cảnh báo nguy hiểm, nhằm đảm bảo ứng dụng có thể phản hồi kịp thời và chính xác trong các tình huống nguy cấp.
%     \item Tích hợp các ngôn ngữ và giọng nói khác nhau trong module text-to-speech, mở rộng khả năng tiếp cận của ứng dụng cho người dùng quốc tế.
%     \item Tiến hành các thử nghiệm thực tế với người dùng thật, thu thập ý kiến phản hồi để cải thiện giao diện và trải nghiệm người dùng.
%     \item Hoàn thiện tài liệu hướng dẫn sử dụng và cung cấp các video hướng dẫn trực quan giúp người dùng (đặc biệt là người khiếm thị) có thể dễ dàng tiếp cận và sử dụng ứng dụng.
% \end{itemize}

% Nhóm tin rằng với các cải tiến và bổ sung này, ứng dụng sẽ không chỉ hỗ trợ người khiếm thị nhận diện môi trường xung quanh mà còn mang lại trải nghiệm tốt nhất, đáp ứng nhu cầu thực tế của người dùng.

\printbibliography

\end{document}
